{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkIvWz9UTU8R",
        "outputId": "5b0b2fe4-3062-477b-924d-83291e7e3d91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flair\n",
            "  Downloading flair-0.13.0-py3-none-any.whl (387 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.2/387.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boto3>=1.20.27 (from flair)\n",
            "  Downloading boto3-1.29.1-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bpemb>=0.3.2 (from flair)\n",
            "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
            "Collecting conllu>=4.0 (from flair)\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Collecting deprecated>=1.2.13 (from flair)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting ftfy>=6.1.0 (from flair)\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.6.6)\n",
            "Requirement already satisfied: gensim>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.3.2)\n",
            "Collecting huggingface-hub>=0.10.0 (from flair)\n",
            "  Downloading huggingface_hub-0.19.3-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting janome>=0.4.2 (from flair)\n",
            "  Downloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect>=1.0.9 (from flair)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.9.3)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from flair) (3.7.1)\n",
            "Requirement already satisfied: more-itertools>=8.13.0 in /usr/local/lib/python3.10/dist-packages (from flair) (10.1.0)\n",
            "Collecting mpld3>=0.3 (from flair)\n",
            "  Downloading mpld3-0.5.9-py3-none-any.whl (201 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.2/201.2 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pptree>=3.1 (from flair)\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from flair) (2.8.2)\n",
            "Collecting pytorch-revgrad>=0.2.0 (from flair)\n",
            "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from flair) (2023.6.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from flair) (1.2.2)\n",
            "Collecting segtok>=1.5.11 (from flair)\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Collecting sqlitedict>=2.0.0 (from flair)\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from flair) (0.9.0)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from flair) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm>=4.63.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.66.1)\n",
            "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair)\n",
            "  Downloading transformer_smaller_training_vocab-0.3.2-py3-none-any.whl (14 kB)\n",
            "Collecting transformers[sentencepiece]<5.0.0,>=4.18.0 (from flair)\n",
            "  Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<2.0.0,>=1.0.0 (from flair)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wikipedia-api>=0.5.7 (from flair)\n",
            "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
            "Collecting semver<4.0.0,>=3.0.0 (from flair)\n",
            "  Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
            "Collecting botocore<1.33.0,>=1.32.1 (from boto3>=1.20.27->flair)\n",
            "  Downloading botocore-1.32.1-py3-none-any.whl (11.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.20.27->flair)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.8.0,>=0.7.0 (from boto3>=1.20.27->flair)\n",
            "  Downloading s3transfer-0.7.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.2->flair) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.2->flair) (2.31.0)\n",
            "Collecting sentencepiece (from bpemb>=0.3.2->flair)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->flair) (1.14.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1.0->flair) (0.2.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair) (3.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair) (4.11.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.2.0->flair) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.2.0->flair) (6.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (2023.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (23.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (4.44.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (3.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mpld3>=0.3->flair) (3.1.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair) (3.2.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (3.2.1)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (2.1.0)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair)\n",
            "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (3.20.3)\n",
            "Collecting accelerate>=0.20.3 (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair)\n",
            "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair) (2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mpld3>=0.3->flair) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.2->flair) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.2->flair) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.2->flair) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.2->flair) (1.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.8,>=1.5.0->flair) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (5.9.5)\n",
            "Building wheels for collected packages: langdetect, pptree, sqlitedict\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=38b01d0c3f92f8e1c48663b2dae427d5d3a91a6426d7b097cad62aecae1a2c40\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4607 sha256=5650e4c474d4023b74623b8d3bdb8fb2ac421c30c7fcb2a37b50fd9b788b86b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/b6/0e/6f26eb9e6eb53ff2107a7888d72b5a6a597593956113037828\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=73815764f9decbf6f08face09323b33f5998f38f098b700a05e741dbeea7af3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "Successfully built langdetect pptree sqlitedict\n",
            "Installing collected packages: sqlitedict, sentencepiece, pptree, janome, urllib3, semver, segtok, safetensors, langdetect, jmespath, ftfy, deprecated, conllu, botocore, wikipedia-api, s3transfer, pytorch-revgrad, mpld3, huggingface-hub, bpemb, tokenizers, boto3, accelerate, transformers, transformer-smaller-training-vocab, flair\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "Successfully installed accelerate-0.24.1 boto3-1.29.1 botocore-1.32.1 bpemb-0.3.4 conllu-4.5.3 deprecated-1.2.14 flair-0.13.0 ftfy-6.1.1 huggingface-hub-0.19.3 janome-0.5.0 jmespath-1.0.1 langdetect-1.0.9 mpld3-0.5.9 pptree-3.1 pytorch-revgrad-0.2.0 s3transfer-0.7.0 safetensors-0.4.0 segtok-1.5.11 semver-3.0.2 sentencepiece-0.1.99 sqlitedict-2.1.0 tokenizers-0.15.0 transformer-smaller-training-vocab-0.3.2 transformers-4.35.2 urllib3-1.26.18 wikipedia-api-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install flair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1XNMhPnz7o6V"
      },
      "outputs": [],
      "source": [
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "from flair.trainers import ModelTrainer\n",
        "from flair.datasets import ColumnCorpus\n",
        "from flair.datasets import SentenceDataset\n",
        "from flair.data import Token\n",
        "from tqdm import tqdm\n",
        "import gensim\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import pandas as pd\n",
        "pd.set_option('max.colwidth', 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AStNcyORVFAh"
      },
      "source": [
        "# Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeBXaVpkAXYW",
        "outputId": "1e9975c4-17d0-4300-c345-896d23605528"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/mypos-ver.3.0-flair.txt .\n",
        "!cp /content/drive/MyDrive/word2vec.model ."
      ],
      "metadata": {
        "id": "tLo1Ylc4AoAU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7PuQenxUC5c",
        "outputId": "9f35eb4a-51ee-470b-8516-df80ed3ceea2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-16 09:19:56,059 Reading data from .\n",
            "2023-11-16 09:19:56,067 Train: mypos-ver.3.0-flair.txt\n",
            "2023-11-16 09:19:56,068 Dev: None\n",
            "2023-11-16 09:19:56,070 Test: None\n",
            "2023-11-16 09:20:11,239 No test split found. Using 0% (i.e. 4320 samples) of the train split as test data\n",
            "2023-11-16 09:20:11,267 No dev split found. Using 0% (i.e. 3888 samples) of the train split as dev data\n"
          ]
        }
      ],
      "source": [
        "dataset_file = \"mypos-ver.3.0-flair.txt\"\n",
        "\n",
        "# Define the columns in dataset\n",
        "columns = {0: 'text', 1: 'pos'}\n",
        "\n",
        "# Initialize the corpus\n",
        "corpus = ColumnCorpus(data_folder='.', column_format=columns, train_file=dataset_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Word Embeddings (word2vec)"
      ],
      "metadata": {
        "id": "TgGprn3tAOxy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0tFeSY6bNSwT"
      },
      "outputs": [],
      "source": [
        "from flair.embeddings import TokenEmbeddings\n",
        "from gensim.models import Word2Vec\n",
        "from flair.data import Sentence\n",
        "from typing import List, Dict, Any\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class CustomWordEmbeddings(TokenEmbeddings):\n",
        "    embeddings_name = \"custom_word_embeddings\"\n",
        "    def __init__(self, model_path):\n",
        "        super().__init__()\n",
        "        self.name = \"custom_word_embeddings\"\n",
        "        self.static_embeddings = True\n",
        "        self.model_path = model_path\n",
        "        self.model = Word2Vec.load(model_path)\n",
        "\n",
        "    @property\n",
        "    def embedding_length(self):\n",
        "        return self.model.vector_size\n",
        "\n",
        "    def embed(self, sentences: List[Sentence]) -> List[Sentence]:\n",
        "        for sentence in sentences:\n",
        "            for token in sentence.tokens:\n",
        "                if token.text in self.model.wv:\n",
        "                    embedding = np.array(self.model.wv[token.text])\n",
        "                    embedding_tensor = torch.tensor(embedding)  # Convert to a PyTorch tensor\n",
        "                    token.set_embedding(self.name, embedding_tensor)\n",
        "                else:\n",
        "                    # Handle out-of-vocabulary words\n",
        "                    token.set_embedding(self.name, torch.zeros(self.model.vector_size))\n",
        "        return sentences\n",
        "\n",
        "    @classmethod\n",
        "    def from_params(cls, params: Dict[str, Any]) -> \"CustomWordEmbeddings\":\n",
        "        model_path = params.get(\"model_path\")\n",
        "        return cls(model_path)\n",
        "\n",
        "    def to_params(self) -> Dict[str, Any]:\n",
        "        return {\"model_path\": self.model_path}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "epqudzHJR-Jy"
      },
      "outputs": [],
      "source": [
        "from flair.embeddings.base import EMBEDDING_CLASSES\n",
        "model_path = \"word2vec.model\"\n",
        "# Create a custom TokenEmbeddings object\n",
        "custom_word_embeddings = CustomWordEmbeddings(model_path)\n",
        "\n",
        "EMBEDDING_CLASSES.update({\n",
        "    \"custom_word_embeddings\": custom_word_embeddings\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYolxd5CVgES"
      },
      "source": [
        "# Sequence Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HcOqkOLDj3z",
        "outputId": "006cbe67-043c-48b6-dc29-4c359da21b76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-16 09:20:11,879 Computing label dictionary. Progress:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n",
            "34989it [00:01, 28574.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-16 09:20:13,165 Dictionary created for label 'pos' with 16 values: part (seen 108378 times), n (seen 85028 times), ppm (seen 70057 times), v (seen 62463 times), punc (seen 43823 times), pron (seen 16539 times), conj (seen 14497 times), adj (seen 12821 times), adv (seen 8663 times), num (seen 4867 times), tn (seen 4734 times), fw (seen 2590 times), int (seen 545 times), abb (seen 283 times), sb (seen 224 times), O (seen 1 times)\n",
            "2023-11-16 09:20:13,169 SequenceTagger predicts: Dictionary with 16 tags: part, n, ppm, v, punc, pron, conj, adj, adv, num, tn, fw, int, abb, sb, O\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-16 09:20:26,519 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:20:26,521 Model: \"SequenceTagger(\n",
            "  (embeddings): CustomWordEmbeddings()\n",
            "  (word_dropout): WordDropout(p=0.05)\n",
            "  (locked_dropout): LockedDropout(p=0.5)\n",
            "  (embedding2nn): Linear(in_features=100, out_features=100, bias=True)\n",
            "  (rnn): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=512, out_features=18, bias=True)\n",
            "  (loss_function): ViterbiLoss()\n",
            "  (crf): CRF()\n",
            ")\"\n",
            "2023-11-16 09:20:26,525 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:20:26,527 Corpus: 34989 train + 3888 dev + 4320 test sentences\n",
            "2023-11-16 09:20:26,532 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:20:26,532 Train:  34989 sentences\n",
            "2023-11-16 09:20:26,536         (train_with_dev=False, train_with_test=False)\n",
            "2023-11-16 09:20:26,538 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:20:26,540 Training Params:\n",
            "2023-11-16 09:20:26,542  - learning_rate: \"0.1\" \n",
            "2023-11-16 09:20:26,543  - mini_batch_size: \"32\"\n",
            "2023-11-16 09:20:26,547  - max_epochs: \"10\"\n",
            "2023-11-16 09:20:26,547  - shuffle: \"True\"\n",
            "2023-11-16 09:20:26,548 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:20:26,549 Plugins:\n",
            "2023-11-16 09:20:26,550  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'\n",
            "2023-11-16 09:20:26,551 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:20:26,552 Final evaluation on model from best epoch (best-model.pt)\n",
            "2023-11-16 09:20:26,553  - metric: \"('micro avg', 'f1-score')\"\n",
            "2023-11-16 09:20:26,554 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:20:26,555 Computation:\n",
            "2023-11-16 09:20:26,556  - compute on device: cuda:0\n",
            "2023-11-16 09:20:26,557  - embedding storage: cpu\n",
            "2023-11-16 09:20:26,558 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:20:26,559 Model training base path: \"pos_tagger_w2v\"\n",
            "2023-11-16 09:20:26,560 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:20:26,562 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:20:36,179 epoch 1 - iter 109/1094 - loss 1.10930883 - time (sec): 9.62 - samples/sec: 4495.72 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:20:43,718 epoch 1 - iter 218/1094 - loss 0.87650244 - time (sec): 17.16 - samples/sec: 5038.02 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:20:51,584 epoch 1 - iter 327/1094 - loss 0.78056461 - time (sec): 25.02 - samples/sec: 5200.85 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:21:00,604 epoch 1 - iter 436/1094 - loss 0.71771525 - time (sec): 34.04 - samples/sec: 5114.46 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:21:10,823 epoch 1 - iter 545/1094 - loss 0.67560558 - time (sec): 44.26 - samples/sec: 4933.71 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:21:19,854 epoch 1 - iter 654/1094 - loss 0.64382540 - time (sec): 53.29 - samples/sec: 4932.62 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:21:27,437 epoch 1 - iter 763/1094 - loss 0.61968788 - time (sec): 60.87 - samples/sec: 5017.91 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:21:35,978 epoch 1 - iter 872/1094 - loss 0.60060386 - time (sec): 69.42 - samples/sec: 5026.77 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:21:42,941 epoch 1 - iter 981/1094 - loss 0.58351221 - time (sec): 76.38 - samples/sec: 5127.91 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:21:51,811 epoch 1 - iter 1090/1094 - loss 0.56853189 - time (sec): 85.25 - samples/sec: 5091.06 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:21:52,074 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:21:52,076 EPOCH 1 done: loss 0.5680 - lr: 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 61/61 [00:10<00:00,  6.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-16 09:22:02,973 DEV : loss 0.26060208678245544 - f1-score (micro avg)  0.9192\n",
            "2023-11-16 09:22:03,238  - 0 epochs without improvement\n",
            "2023-11-16 09:22:03,243 saving best model\n",
            "2023-11-16 09:22:03,257 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:22:11,944 epoch 2 - iter 109/1094 - loss 0.42815902 - time (sec): 8.68 - samples/sec: 4972.78 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:22:19,491 epoch 2 - iter 218/1094 - loss 0.42612937 - time (sec): 16.23 - samples/sec: 5343.09 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:22:28,558 epoch 2 - iter 327/1094 - loss 0.42069821 - time (sec): 25.30 - samples/sec: 5119.10 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:22:35,848 epoch 2 - iter 436/1094 - loss 0.41786099 - time (sec): 32.59 - samples/sec: 5315.95 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:22:45,289 epoch 2 - iter 545/1094 - loss 0.41520458 - time (sec): 42.03 - samples/sec: 5156.42 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:22:52,614 epoch 2 - iter 654/1094 - loss 0.41249147 - time (sec): 49.35 - samples/sec: 5267.58 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:23:01,917 epoch 2 - iter 763/1094 - loss 0.40959196 - time (sec): 58.66 - samples/sec: 5174.74 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:23:09,451 epoch 2 - iter 872/1094 - loss 0.40851153 - time (sec): 66.19 - samples/sec: 5249.25 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:23:19,463 epoch 2 - iter 981/1094 - loss 0.40724518 - time (sec): 76.20 - samples/sec: 5129.60 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:23:26,768 epoch 2 - iter 1090/1094 - loss 0.40596212 - time (sec): 83.51 - samples/sec: 5198.14 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:23:27,059 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:23:27,061 EPOCH 2 done: loss 0.4059 - lr: 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 61/61 [00:16<00:00,  3.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-16 09:23:44,213 DEV : loss 0.22997337579727173 - f1-score (micro avg)  0.929\n",
            "2023-11-16 09:23:44,370  - 0 epochs without improvement\n",
            "2023-11-16 09:23:44,372 saving best model\n",
            "2023-11-16 09:23:44,384 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:23:53,729 epoch 3 - iter 109/1094 - loss 0.38229644 - time (sec): 9.34 - samples/sec: 4555.44 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:24:01,330 epoch 3 - iter 218/1094 - loss 0.38675175 - time (sec): 16.94 - samples/sec: 5057.32 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:24:10,231 epoch 3 - iter 327/1094 - loss 0.38206419 - time (sec): 25.84 - samples/sec: 5012.62 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:24:18,666 epoch 3 - iter 436/1094 - loss 0.38285586 - time (sec): 34.28 - samples/sec: 5053.55 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:24:27,394 epoch 3 - iter 545/1094 - loss 0.38165484 - time (sec): 43.01 - samples/sec: 5052.47 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:24:36,052 epoch 3 - iter 654/1094 - loss 0.38034570 - time (sec): 51.67 - samples/sec: 5049.31 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:24:44,711 epoch 3 - iter 763/1094 - loss 0.37870314 - time (sec): 60.32 - samples/sec: 5047.94 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:24:53,129 epoch 3 - iter 872/1094 - loss 0.37657065 - time (sec): 68.74 - samples/sec: 5049.70 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:25:00,818 epoch 3 - iter 981/1094 - loss 0.37575600 - time (sec): 76.43 - samples/sec: 5107.64 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:25:09,760 epoch 3 - iter 1090/1094 - loss 0.37428999 - time (sec): 85.37 - samples/sec: 5085.84 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:25:09,960 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:25:09,961 EPOCH 3 done: loss 0.3743 - lr: 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 61/61 [00:14<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-16 09:25:25,132 DEV : loss 0.21252262592315674 - f1-score (micro avg)  0.9349\n",
            "2023-11-16 09:25:25,283  - 0 epochs without improvement\n",
            "2023-11-16 09:25:25,285 saving best model\n",
            "2023-11-16 09:25:25,295 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:25:32,480 epoch 4 - iter 109/1094 - loss 0.35279117 - time (sec): 7.18 - samples/sec: 5992.53 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:25:41,801 epoch 4 - iter 218/1094 - loss 0.35311795 - time (sec): 16.50 - samples/sec: 5221.74 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:25:49,325 epoch 4 - iter 327/1094 - loss 0.35470020 - time (sec): 24.03 - samples/sec: 5390.21 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:25:57,779 epoch 4 - iter 436/1094 - loss 0.35446758 - time (sec): 32.48 - samples/sec: 5314.28 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:26:05,531 epoch 4 - iter 545/1094 - loss 0.35549979 - time (sec): 40.23 - samples/sec: 5357.12 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:26:13,542 epoch 4 - iter 654/1094 - loss 0.35661193 - time (sec): 48.24 - samples/sec: 5365.74 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:26:25,713 epoch 4 - iter 763/1094 - loss 0.35656123 - time (sec): 60.42 - samples/sec: 5013.62 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:26:34,673 epoch 4 - iter 872/1094 - loss 0.35659039 - time (sec): 69.38 - samples/sec: 4995.23 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:26:43,311 epoch 4 - iter 981/1094 - loss 0.35615136 - time (sec): 78.01 - samples/sec: 5000.03 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:26:51,256 epoch 4 - iter 1090/1094 - loss 0.35601284 - time (sec): 85.96 - samples/sec: 5050.59 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:26:51,586 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:26:51,589 EPOCH 4 done: loss 0.3561 - lr: 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 61/61 [00:15<00:00,  4.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-16 09:27:07,521 DEV : loss 0.2078721523284912 - f1-score (micro avg)  0.9365\n",
            "2023-11-16 09:27:07,778  - 0 epochs without improvement\n",
            "2023-11-16 09:27:07,783 saving best model\n",
            "2023-11-16 09:27:07,799 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:27:17,536 epoch 5 - iter 109/1094 - loss 0.35136357 - time (sec): 9.73 - samples/sec: 4540.26 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:27:24,505 epoch 5 - iter 218/1094 - loss 0.35462983 - time (sec): 16.70 - samples/sec: 5219.61 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:27:34,032 epoch 5 - iter 327/1094 - loss 0.35525298 - time (sec): 26.23 - samples/sec: 4964.96 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:27:41,052 epoch 5 - iter 436/1094 - loss 0.35326932 - time (sec): 33.25 - samples/sec: 5204.35 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:27:50,541 epoch 5 - iter 545/1094 - loss 0.35302567 - time (sec): 42.74 - samples/sec: 5073.88 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:27:58,496 epoch 5 - iter 654/1094 - loss 0.35325018 - time (sec): 50.69 - samples/sec: 5147.50 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:28:08,236 epoch 5 - iter 763/1094 - loss 0.35164840 - time (sec): 60.43 - samples/sec: 5039.41 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:28:15,481 epoch 5 - iter 872/1094 - loss 0.34996606 - time (sec): 67.68 - samples/sec: 5132.30 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:28:25,304 epoch 5 - iter 981/1094 - loss 0.34948040 - time (sec): 77.50 - samples/sec: 5046.10 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:28:32,718 epoch 5 - iter 1090/1094 - loss 0.34856077 - time (sec): 84.91 - samples/sec: 5111.92 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:28:32,984 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:28:32,986 EPOCH 5 done: loss 0.3486 - lr: 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 61/61 [00:14<00:00,  4.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-16 09:28:48,156 DEV : loss 0.19975405931472778 - f1-score (micro avg)  0.9399\n",
            "2023-11-16 09:28:48,313  - 0 epochs without improvement\n",
            "2023-11-16 09:28:48,314 saving best model\n",
            "2023-11-16 09:28:48,327 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:28:55,902 epoch 6 - iter 109/1094 - loss 0.34820990 - time (sec): 7.57 - samples/sec: 5760.70 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:29:05,246 epoch 6 - iter 218/1094 - loss 0.34250507 - time (sec): 16.92 - samples/sec: 5126.20 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:29:12,694 epoch 6 - iter 327/1094 - loss 0.34224762 - time (sec): 24.36 - samples/sec: 5359.07 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:29:22,063 epoch 6 - iter 436/1094 - loss 0.34071287 - time (sec): 33.73 - samples/sec: 5140.17 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:29:29,044 epoch 6 - iter 545/1094 - loss 0.34059740 - time (sec): 40.71 - samples/sec: 5313.59 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:29:39,053 epoch 6 - iter 654/1094 - loss 0.34195318 - time (sec): 50.72 - samples/sec: 5130.45 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:29:46,391 epoch 6 - iter 763/1094 - loss 0.34169445 - time (sec): 58.06 - samples/sec: 5236.20 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:29:56,398 epoch 6 - iter 872/1094 - loss 0.34144661 - time (sec): 68.07 - samples/sec: 5105.27 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:30:04,813 epoch 6 - iter 981/1094 - loss 0.34066043 - time (sec): 76.48 - samples/sec: 5113.49 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:30:14,501 epoch 6 - iter 1090/1094 - loss 0.33980864 - time (sec): 86.17 - samples/sec: 5037.95 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:30:14,884 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:30:14,885 EPOCH 6 done: loss 0.3398 - lr: 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 61/61 [00:15<00:00,  3.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-16 09:30:31,549 DEV : loss 0.19904547929763794 - f1-score (micro avg)  0.9408\n",
            "2023-11-16 09:30:31,812  - 0 epochs without improvement\n",
            "2023-11-16 09:30:31,814 saving best model\n",
            "2023-11-16 09:30:31,828 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:30:39,112 epoch 7 - iter 109/1094 - loss 0.33127178 - time (sec): 7.28 - samples/sec: 5871.15 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:30:47,748 epoch 7 - iter 218/1094 - loss 0.33657902 - time (sec): 15.92 - samples/sec: 5459.21 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:30:55,722 epoch 7 - iter 327/1094 - loss 0.33615405 - time (sec): 23.89 - samples/sec: 5418.61 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:31:03,950 epoch 7 - iter 436/1094 - loss 0.33694499 - time (sec): 32.12 - samples/sec: 5395.48 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:31:12,899 epoch 7 - iter 545/1094 - loss 0.33482176 - time (sec): 41.07 - samples/sec: 5257.96 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:31:20,528 epoch 7 - iter 654/1094 - loss 0.33468367 - time (sec): 48.70 - samples/sec: 5317.28 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:31:30,217 epoch 7 - iter 763/1094 - loss 0.33460434 - time (sec): 58.39 - samples/sec: 5194.25 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:31:37,682 epoch 7 - iter 872/1094 - loss 0.33376965 - time (sec): 65.85 - samples/sec: 5261.78 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:31:47,196 epoch 7 - iter 981/1094 - loss 0.33329764 - time (sec): 75.37 - samples/sec: 5181.48 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:31:55,121 epoch 7 - iter 1090/1094 - loss 0.33426499 - time (sec): 83.29 - samples/sec: 5214.03 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:31:55,423 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:31:55,426 EPOCH 7 done: loss 0.3342 - lr: 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 61/61 [00:13<00:00,  4.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-16 09:32:09,585 DEV : loss 0.19517748057842255 - f1-score (micro avg)  0.941\n",
            "2023-11-16 09:32:09,743  - 0 epochs without improvement\n",
            "2023-11-16 09:32:09,744 saving best model\n",
            "2023-11-16 09:32:09,756 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:32:18,939 epoch 8 - iter 109/1094 - loss 0.32555982 - time (sec): 9.18 - samples/sec: 4626.27 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:32:26,673 epoch 8 - iter 218/1094 - loss 0.33085183 - time (sec): 16.91 - samples/sec: 5091.98 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:32:35,224 epoch 8 - iter 327/1094 - loss 0.32803467 - time (sec): 25.47 - samples/sec: 5085.33 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:32:45,147 epoch 8 - iter 436/1094 - loss 0.32905347 - time (sec): 35.39 - samples/sec: 4885.89 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:32:55,257 epoch 8 - iter 545/1094 - loss 0.32854627 - time (sec): 45.50 - samples/sec: 4759.38 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:33:02,723 epoch 8 - iter 654/1094 - loss 0.32881285 - time (sec): 52.96 - samples/sec: 4900.19 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:33:11,756 epoch 8 - iter 763/1094 - loss 0.33006718 - time (sec): 62.00 - samples/sec: 4879.87 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:33:20,646 epoch 8 - iter 872/1094 - loss 0.32978510 - time (sec): 70.89 - samples/sec: 4896.97 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:33:30,185 epoch 8 - iter 981/1094 - loss 0.32911586 - time (sec): 80.43 - samples/sec: 4861.21 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:33:38,023 epoch 8 - iter 1090/1094 - loss 0.32945046 - time (sec): 88.26 - samples/sec: 4919.81 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:33:38,249 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:33:38,251 EPOCH 8 done: loss 0.3295 - lr: 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 61/61 [00:17<00:00,  3.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-16 09:33:55,739 DEV : loss 0.19180220365524292 - f1-score (micro avg)  0.9416\n",
            "2023-11-16 09:33:55,896  - 0 epochs without improvement\n",
            "2023-11-16 09:33:55,898 saving best model\n",
            "2023-11-16 09:33:55,908 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:34:04,398 epoch 9 - iter 109/1094 - loss 0.31988500 - time (sec): 8.49 - samples/sec: 5087.53 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:34:13,052 epoch 9 - iter 218/1094 - loss 0.32251950 - time (sec): 17.14 - samples/sec: 5028.37 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:34:21,403 epoch 9 - iter 327/1094 - loss 0.32382144 - time (sec): 25.49 - samples/sec: 5091.02 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:34:29,748 epoch 9 - iter 436/1094 - loss 0.32530121 - time (sec): 33.84 - samples/sec: 5109.07 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:34:37,171 epoch 9 - iter 545/1094 - loss 0.32506888 - time (sec): 41.26 - samples/sec: 5232.29 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:34:46,203 epoch 9 - iter 654/1094 - loss 0.32491287 - time (sec): 50.29 - samples/sec: 5153.82 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:34:54,012 epoch 9 - iter 763/1094 - loss 0.32549227 - time (sec): 58.10 - samples/sec: 5212.93 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:35:03,341 epoch 9 - iter 872/1094 - loss 0.32482160 - time (sec): 67.43 - samples/sec: 5142.05 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:35:11,015 epoch 9 - iter 981/1094 - loss 0.32441622 - time (sec): 75.10 - samples/sec: 5201.05 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:35:20,720 epoch 9 - iter 1090/1094 - loss 0.32522415 - time (sec): 84.81 - samples/sec: 5118.46 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:35:20,955 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:35:20,956 EPOCH 9 done: loss 0.3252 - lr: 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 61/61 [00:12<00:00,  4.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-16 09:35:34,515 DEV : loss 0.19097286462783813 - f1-score (micro avg)  0.9417\n",
            "2023-11-16 09:35:34,781  - 0 epochs without improvement\n",
            "2023-11-16 09:35:34,786 saving best model\n",
            "2023-11-16 09:35:34,796 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:35:42,645 epoch 10 - iter 109/1094 - loss 0.32932808 - time (sec): 7.85 - samples/sec: 5478.52 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:35:51,809 epoch 10 - iter 218/1094 - loss 0.33152024 - time (sec): 17.01 - samples/sec: 5098.71 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:36:00,544 epoch 10 - iter 327/1094 - loss 0.32824419 - time (sec): 25.74 - samples/sec: 5088.37 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:36:09,149 epoch 10 - iter 436/1094 - loss 0.32806077 - time (sec): 34.35 - samples/sec: 5074.84 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:36:17,614 epoch 10 - iter 545/1094 - loss 0.32719729 - time (sec): 42.81 - samples/sec: 5096.76 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:36:25,447 epoch 10 - iter 654/1094 - loss 0.32451085 - time (sec): 50.65 - samples/sec: 5157.07 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:36:33,885 epoch 10 - iter 763/1094 - loss 0.32365173 - time (sec): 59.09 - samples/sec: 5142.35 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:36:41,088 epoch 10 - iter 872/1094 - loss 0.32312307 - time (sec): 66.29 - samples/sec: 5235.29 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:36:50,407 epoch 10 - iter 981/1094 - loss 0.32280534 - time (sec): 75.61 - samples/sec: 5164.86 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:36:57,591 epoch 10 - iter 1090/1094 - loss 0.32201557 - time (sec): 82.79 - samples/sec: 5243.56 - lr: 0.100000 - momentum: 0.000000\n",
            "2023-11-16 09:36:57,835 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:36:57,837 EPOCH 10 done: loss 0.3221 - lr: 0.100000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 61/61 [00:16<00:00,  3.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-16 09:37:14,888 DEV : loss 0.19604137539863586 - f1-score (micro avg)  0.9418\n",
            "2023-11-16 09:37:15,046  - 0 epochs without improvement\n",
            "2023-11-16 09:37:15,048 saving best model\n",
            "2023-11-16 09:37:15,064 ----------------------------------------------------------------------------------------------------\n",
            "2023-11-16 09:37:15,066 Loading model from best epoch ...\n",
            "2023-11-16 09:37:15,421 SequenceTagger predicts: Dictionary with 18 tags: part, n, ppm, v, punc, pron, conj, adj, adv, num, tn, fw, int, abb, sb, O, <START>, <STOP>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 68/68 [00:12<00:00,  5.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-16 09:37:27,968 \n",
            "Results:\n",
            "- F-score (micro) 0.9381\n",
            "- F-score (macro) 0.8474\n",
            "- Accuracy 0.9381\n",
            "\n",
            "By class:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        part     0.9383    0.9477    0.9430     13207\n",
            "           n     0.9157    0.9196    0.9176     10494\n",
            "         ppm     0.9630    0.9875    0.9751      8638\n",
            "           v     0.9364    0.9398    0.9381      7588\n",
            "        punc     0.9958    0.9959    0.9958      5421\n",
            "        pron     0.9600    0.9711    0.9655      2004\n",
            "        conj     0.8836    0.9196    0.9012      1741\n",
            "         adj     0.8172    0.7541    0.7843      1541\n",
            "         adv     0.8883    0.7629    0.8209      1063\n",
            "         num     0.9444    0.9413    0.9429       596\n",
            "          tn     0.9463    0.9511    0.9487       593\n",
            "          fw     0.8389    0.3463    0.4902       361\n",
            "         int     0.8000    0.6471    0.7154        68\n",
            "         abb     1.0000    0.5000    0.6667        38\n",
            "          sb     0.8571    0.6000    0.7059        20\n",
            "\n",
            "    accuracy                         0.9381     53373\n",
            "   macro avg     0.9123    0.8123    0.8474     53373\n",
            "weighted avg     0.9373    0.9381    0.9368     53373\n",
            "\n",
            "2023-11-16 09:37:27,969 ----------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'test_score': 0.9381335131995578}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "label_type = 'pos'\n",
        "\n",
        "# Create a label dictionary from the corpus\n",
        "label_dict = corpus.make_label_dictionary(label_type=label_type)\n",
        "\n",
        "# Create model\n",
        "model = SequenceTagger(hidden_size=256,\n",
        "                      embeddings=custom_word_embeddings,\n",
        "                      tag_dictionary=label_dict,\n",
        "                      tag_type=label_type)\n",
        "\n",
        "# Create the trainer and train the model\n",
        "trainer = ModelTrainer(model, corpus)\n",
        "trainer.train('pos_tagger_w2v', learning_rate=0.1, mini_batch_size=32, max_epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict"
      ],
      "metadata": {
        "id": "IeAE-YjhgaE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model you trained\n",
        "model = SequenceTagger.load('pos_tagger_w2v/final-model.pt')\n",
        "\n",
        "# create example sentence\n",
        "sentence = Sentence('ကျွန်တော့် အတွက် သိပ် အဆင်မပြေ လှ ပါ ဘူး ။')\n",
        "\n",
        "# predict tags and print\n",
        "model.predict(sentence)\n",
        "\n",
        "print(sentence.to_tagged_string())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfxgYDuAgTX5",
        "outputId": "c116e4f4-9f89-462a-d0ae-2d2404344897"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-16 09:37:28,321 SequenceTagger predicts: Dictionary with 18 tags: part, n, ppm, v, punc, pron, conj, adj, adv, num, tn, fw, int, abb, sb, O, <START>, <STOP>\n",
            "Sentence[8]: \"ကျွန်တော့် အတွက် သိပ် အဆင်မပြေ လှ ပါ ဘူး ။\" → [\"ကျွန်တော့်\"/pron, \"အတွက်\"/ppm, \"သိပ်\"/adv, \"အဆင်မပြေ\"/v, \"လှ\"/part, \"ပါ\"/part, \"ဘူး\"/part, \"။\"/punc]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv pos_tagger_w2v /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "sWXFIhozHeHU"
      },
      "execution_count": 10,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}